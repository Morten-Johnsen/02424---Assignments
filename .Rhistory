Lik.fun.alldata <- function(theta,y,sigma2){
exp(sum(-(y-theta)^2/(2*sigma2)))
}
theta.vals <- seq(50,110,0.1)
lik.fun.vals <- rep(0,length(theta.vals))
for (i in 1:length(theta.vals)){
lik.fun.vals[i] <- Lik.fun.alldata(theta.vals[i],y,sigma2)
}
plot(theta.vals,lik.fun.vals/max(lik.fun.vals))
# Now use only average
ybar <- mean(y)
Lik.fun.avg <- function(theta,ybar,sigma2){
exp(sum(-(ybar-theta)^2/(2*sigma2)))
}
lik.fun.vals.avg <- rep(0,length(theta.vals))
for (i in 1:length(theta.vals)){
lik.fun.vals.avg[i] <- Lik.fun.avg(theta.vals[i],y,sigma2)
}
# Plot
lines(theta.vals,lik.fun.vals.avg/max(lik.fun.vals.avg),col=2)
# They are the same, so I am satisfied :)
# Find MLE by optimization
Lik.fun <- function(theta,y){
prod(dnorm(y,mean=theta,sd=sigma2))
}
opt.fit <- nlminb(c(ybar),Lik.fun,y=y)
opt.fit
MLE <- opt.fit$objective
thetahat <- opt.fit$par
thetahat
ybar
opt.fit <- nlminb(c(70),Lik.fun,y=y)
MLE <- opt.fit$objective
thetahat <- opt.fit$par
thetahat
opt.fit$objective
opt.fit <- nlminb(c(70),Lik.fun,y=y)
MLE <- opt.fit$objective
opt.fit
opt.fit <- nlminb(c(75),Lik.fun,y=y)
opt.fit
opt.fit <- nlminb(c(75),Lik.fun,y=y)
# Find MLE by optimization
Lik.fun <- function(theta,y){
-prod(dnorm(y,mean=theta,sd=sigma2))
}
opt.fit <- nlminb(c(75),Lik.fun,y=y)
opt.fit
# log likelihood
ll.fun <- function(theta,y){
sum(dnorm(y,mean=theta,sd=sigma2,log=TRUE))
}
# log likelihood
ll.fun <- function(theta,y){
-sum(dnorm(y,mean=theta,sd=sigma2,log=TRUE))
}
opt.fit <- nlminb(c(75),ll.fun,y=y)
opt.fit
# Find MLE by optimization
# using log-likelihood:
library(numDeriv)
# log likelihood
ll.fun <- function(theta,y){
-sum(dnorm(y,mean=theta,sd=sigma2,log=TRUE))
}
# log likelihood
ll.fun <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dnorm(y,mean=theta,sd=sigma2,log=TRUE))
}
opt.fit <- nlminb(c(75),ll.fun,y=y)
opt.fit
MLE <- opt.fit$objective
thetahat <- likfun(MLE)
thetahat <- opt.fit$par
thetahat
MLE
# Observed Information
-hessian(ll.fun,thetahat,y=y)
y <- c(4, 6, 3, 7, 2, 4)
ll.fun3 <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dpois(y,lambda=theta,log=TRUE))
}
lambdas <- seq(0.001,10,0.01)
lambdas
plot(beta1,-sapply(c(lambdas,y),ll.fun3),type="l")  # Normalize the likelihood
plot(lambdas,-sapply(c(lambdas,y),ll.fun3),type="l")  # Normalize the likelihood
plot(lambdas,-sapply(lambdas,ll.fun3,y=y),type="l")  # Normalize the likelihood
lambdas <- seq(0.01,10,0.01)
plot(lambdas,-sapply(lambdas,ll.fun3,y=y),type="l")  # Normalize the likelihood
lambdas <- seq(0.1,10,0.01)
plot(lambdas,-sapply(lambdas,ll.fun3,y=y),type="l")  # Normalize the likelihood
lambdas <- seq(1,10,0.01)
lambdas <- seq(1,10,0.01)
plot(lambdas,-sapply(lambdas,ll.fun3,y=y),type="l")  # Normalize the likelihood
y <- c(4, 6, 3, 7, 2, 4)
ll.fun3 <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dpois(y,lambda=theta,log=TRUE))
}
lambdas <- seq(1,10,0.01)
plot(lambdas,-sapply(lambdas,ll.fun3,y=y),type="l")  # Normalize the likelihood
ll.vals3 <- -sapply(lambdas,ll.fun3,y=y)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")  # Normalize the likelihood
lambdas <- seq(1,10,0.01)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")  # Normalize the likelihood
lambdas <- seq(2,8,0.01)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")  # Normalize the likelihood
y <- c(4, 6, 3, 7, 2, 4)
ll.fun3 <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dpois(y,lambda=theta,log=TRUE))
}
ll.vals3 <- -sapply(lambdas,ll.fun3,y=y)
lambdas <- seq(2,8,0.01)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")  # Normalize the likelihood
ll.vals3.exp <- exp(ll.vals3)
ll.vals3.exp
plot(lambdas,ll.vals3.exp/max(ll.vals3.exp),type="l")  # Normalize the likelihood
# Plot normalized likelihood
ll.vals3.exp <- exp(ll.vals3)
plot(lambdas,ll.vals3.exp/max(ll.vals3.exp),type="l")
y <- c(71, 74, 82, 76, 91, 82, 82, 75, 79, 82, 72, 90)
sigma2 <- var(y)
# I plot the likelihood-function but without the scaling constant (because we know sigma^2).
# Thus, I ONLY use the last exponential part
Lik.fun.alldata <- function(theta,y,sigma2){
exp(sum(-(y-theta)^2/(2*sigma2)))
}
theta.vals <- seq(50,110,0.1)
lik.fun.vals <- rep(0,length(theta.vals))
for (i in 1:length(theta.vals)){
lik.fun.vals[i] <- Lik.fun.alldata(theta.vals[i],y,sigma2)
}
plot(theta.vals,lik.fun.vals/max(lik.fun.vals))
y <- c(71, 74, 82, 76, 91, 82, 82, 75, 79, 82, 72, 90)
sigma2 <- var(y)
# I plot the likelihood-function but without the scaling constant (because we know sigma^2).
# Thus, I ONLY use the last exponential part
Lik.fun.alldata <- function(theta,y,sigma2){
exp(sum(-(y-theta)^2/(2*sigma2)))
}
theta.vals <- seq(50,110,0.1)
lik.fun.vals <- rep(0,length(theta.vals))
for (i in 1:length(theta.vals)){
lik.fun.vals[i] <- Lik.fun.alldata(theta.vals[i],y,sigma2)
}
plot(theta.vals,lik.fun.vals/max(lik.fun.vals))
# Now use only average
ybar <- mean(y)
Lik.fun.avg <- function(theta,ybar,sigma2){
exp(sum(-(ybar-theta)^2/(2*sigma2)))
}
lik.fun.vals.avg <- rep(0,length(theta.vals))
for (i in 1:length(theta.vals)){
lik.fun.vals.avg[i] <- Lik.fun.avg(theta.vals[i],y,sigma2)
}
# Plot
lines(theta.vals,lik.fun.vals.avg/max(lik.fun.vals.avg),col=2)
# Find MLE by optimization
# using log-likelihood:
library(numDeriv)
# log likelihood
ll.fun <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dnorm(y,mean=theta,sd=sigma2,log=TRUE))
}
opt.fit <- nlminb(c(75),ll.fun,y=y)
MLE <- opt.fit$objective
thetahat <- opt.fit$par
# Observed Information
-hessian(ll.fun,thetahat,y=y)
# log likelihood
ll.fun <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dnorm(y,mean=theta,sd=sqrt(sigma2),log=TRUE))
}
opt.fit <- nlminb(c(75),ll.fun,y=y)
MLE <- opt.fit$objective
thetahat <- opt.fit$par
thetahat
# Observed Information
-hessian(ll.fun,thetahat,y=y)
y <- c(4, 6, 3, 7, 2, 4)
ll.fun3 <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dpois(y,lambda=theta,log=TRUE))
}
ll.vals3 <- -sapply(lambdas,ll.fun3,y=y)
# Plot log-likelihood with max at 0
lambdas <- seq(2,8,0.01)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")
y <- c(4, 6, 3, 7, 2, 4)
ll.fun3 <- function(theta,y){
# remember a minus, because the optimization function will minimize!
-sum(dpois(y,lambda=theta,log=TRUE))
}
ll.vals3 <- -sapply(lambdas,ll.fun3,y=y)
# Plot log-likelihood with max at 0
lambdas <- seq(2,8,0.01)
plot(lambdas,ll.vals3-max(ll.vals3),type="l")
# Plot normalized likelihood
ll.vals3.exp <- exp(ll.vals3)
plot(lambdas,ll.vals3.exp/max(ll.vals3.exp),type="l")
## Exercises week 4 ----------------------------------
rm(list=ls())
y1 <- c(0.22,0.38,0.72)
y2 <- c(0.31,0.66,0.99)
coal <- c(1,2,3)
y1 <- c(0.22,0.38,0.72)
y2 <- c(0.31,0.66,0.99)
coal <- c(1,2,3)
fit1 <- lm(y1~coal)
summary(fit1)
fit2 <- lm(y2~coal)
summary(fit2)
##################################################
# Ex 3.4
##################################################
x <- rep(1:3,2)
method <- c(rep(0,3),rep(1,3))
y <- c(0.22,0.38,0.72,0.31,0.66,0.99)
method
plot(y ~ x, col=method+1, pch = method+1)
v <- x^((method<=1)+(method==0))
v
Sigma <- diag(v)
# Parameter estimates
X0 <- cbind(1, method, x, method * x)
ISigma <- solve(Sigma)
beta0 <- solve(t(X0) %*% ISigma %*% X0) %*%
t(X0) %*% ISigma %*% y
# Estimated (hat-)values
yh <- X0%*%beta0
sigma.h2 <- t(y-yh)%*%ISigma%*%(y-yh)/
(6-4)
sigma.h2
# With R (lm)
M0 <- lm(y ~ factor(method)*x,weights=1/v)
beta0
coef(M0)
factor(method)
?lm
y1 <- c(0.22,0.38,0.72)
y2 <- c(0.31,0.66,0.99)
x <- c(1,2,3)
v1 <- x^2
v2 <- x
fit1 <- lm(y1~x)
## Exercises week 4 ----------------------------------
rm(list=ls())
y1 <- c(0.22,0.38,0.72)
y2 <- c(0.31,0.66,0.99)
x <- c(1,2,3)
v1 <- x^2
v2 <- x
fit1 <- lm(y1~x)
summary(fit1)
fit1 <- lm(y1~x,weights=1/v1)
summary(fit1)
fit2 <- lm(y2~coal,weights=1/v2)
fit2 <- lm(y2~x,weights=1/v2)
summary(fit2)
fit2 <- lm(y2~x,weights=1/v2)
summary(fit2)
summary(fit1)
##################################################
# Ex 3.4
##################################################
x <- rep(1:3,2)
method <- c(rep(0,3),rep(1,3))
y <- c(0.22,0.38,0.72,0.31,0.66,0.99)
plot(y ~ x, col=method+1, pch = method+1)
v <- x^((method<=1)+(method==0))
Sigma <- diag(v)
# With R (lm)
M0 <- lm(y ~ factor(method)*x,weights=1/v)
beta0
coef(M0)
# Parameter estimates
X0 <- cbind(1, method, x, method * x)
ISigma <- solve(Sigma)
beta0 <- solve(t(X0) %*% ISigma %*% X0) %*%
t(X0) %*% ISigma %*% y
# Estimated (hat-)values
yh <- X0%*%beta0
sigma.h2 <- t(y-yh)%*%ISigma%*%(y-yh)/
(6-4)
sigma.h2
# With R (lm)
M0 <- lm(y ~ factor(method)*x,weights=1/v)
beta0
coef(M0)
## Simplify model
M1 <- lm(y ~ factor(method):x,weights=1/v)
M2 <- lm(y ~ x,weights=1/v)
summary(M2)
## Simplify model
M1 <- lm(y ~ factor(method):x,weights=1/v)
summary(M1)
# And test
anova(M1,M0)
anova(M2,M0)
# Exercise 3.7 (book) ---------------------------------------------------------
data(anscombe)
# Exercise 3.7 (book) ---------------------------------------------------------
rm(list=ls())
data(anscombe)
force(anscombe)
y <- cbind(anscombe$y1,anscombe$y2)
y
y <- rbind(anscombe$y1,anscombe$y2)
y
y <- c(anscombe$y1,anscombe$y2)
y
y <- c(anscombe$y1,anscombe$y2,anscombe$y3,anscombe$y4)
y
x <- c(anscombe$x1,anscombe$x2,anscombe$x3,anscombe$x4)
y <- c(anscombe$y1,anscombe$y2,anscombe$y3,anscombe$y4)
data <- rep(1:4,each=length(anscombe$x2))
data
plot(y ~ x, col=data+1, pch = data+1)
x
plot(anscombe$y1 ~ anscombe$x1, col=1, pch = 1)
par(mfrow=c(2,2))
plot(anscombe$y1 ~ anscombe$x1, col=1, pch = 1)
plot(anscombe$y2 ~ anscombe$x2, col=2, pch = 2)
plot(anscombe$y3 ~ anscombe$x3, col=3, pch = 3)
plot(anscombe$y4 ~ anscombe$x4, col=4, pch = 4)
##################################################
# Exercise 3.7
##################################################
data(anscombe)
M1 <- lm(y1 ~ x1, data=anscombe)
M2 <- lm(y2 ~ x2, data=anscombe)
M3 <- lm(y3 ~ x3, data=anscombe)
M4 <- lm(y4 ~ x4, data = anscombe)
summary(M1)
summary(M2)
summary(M3)
summary(M4)
# but have a look at residual plots
par(mfrow=c(2,2))
plot(M1) # ok.
par(mfrow=c(2,2))
plot(M2) # Not ok, missing a second order term
plot(anscombe$x2,residuals(M2))
M22 <- update(M2,.~.+I(x2^2))
par(mfrow=c(2,2))
plot(M22)
par(mfrow=c(2,2))
plot(M3) # Not ok. one outlier
plot(anscombe$x3,anscombe$y3)
par(mfrow=c(2,2))
plot(M4) # Not ok, no exertation of covariate
plot(y4~x4,data = anscombe)
plot(y1~x1,data = anscombe)
plot(y2~x2,data = anscombe)
plot(y3~x3,data = anscombe)
plot(y4~x4,data = anscombe)
# but have a look at residual plots
par(mfrow=c(2,2))
plot(M1) # ok.
par(mfrow=c(2,2))
plot(M2) # Not ok, missing a second order term
plot(anscombe$x2,residuals(M2))
M22 <- update(M2,.~.+I(x2^2))
par(mfrow=c(2,2))
plot(M22)
par(mfrow=c(2,2))
plot(M3) # Not ok. one outlier
plot(anscombe$x3,anscombe$y3)
par(mfrow=c(2,2))
plot(M4) # Not ok, no exertation of covariate
plot(y4~x4,data = anscombe)
plot(y1~x1,data = anscombe)
plot(y2~x2,data = anscombe)
plot(y3~x3,data = anscombe)
par(mfrow=c(2,2))
plot(y1~x1,data = anscombe)
plot(y2~x2,data = anscombe)
plot(y3~x3,data = anscombe)
plot(y4~x4,data = anscombe)
m1 <- lm(anscombe$y1 ~ anscombe$x1)
m2 <- lm(anscombe$y2 ~ anscombe$x2)
m3 <- lm(anscombe$y3 ~ anscombe$x3)
m4 <- lm(anscombe$y4 ~ anscombe$x4)
plot(m1)
plot(m1) # OK
plot(m2)
plot(m3)
plot(m4)
rm(list=ls())
library(GGally)
library(qqplotr)
library(corrplot)
library(car)
library(reshape)
library(tidyverse)
library(magrittr)
library(dplyr)
library(betareg)
library(statmod)
library(jtools)
library(numDeriv)
library(latex2exp)
library(broom.mixed)
library(car)
library(gridExtra)
library(MASS)
library(data.table)
if (Sys.getenv('USER') == "mortenjohnsen"){
setwd("/Users/mortenjohnsen/OneDrive - Danmarks Tekniske Universitet/DTU/10. Semester/02424 - Advanced Dataanalysis and Statistical Modellling/02424---Assignments/")
}else if (Sys.getenv('USER') == "freja"){
setwd("~/Documents/Uni/TiendeSemester/Adv. data analysis and stat. modelling/02424---Assignments")
}else{
setwd("C:/Users/catdu/OneDrive/DTU/10. semester/Advanced Dataanalysis and Statistical Modelling/Assignment 1/02424---Assignments/")
}
# Look at data
data <- as.data.table(read.table("Assignment 2/earinfect.txt", header = T))
summary(data)
data[,sum(persons), by = swimmer]
data[, sum(persons), by =location]
data[, sum(persons), by=age]
data[, sum(persons), by = sex]
data[persons == max(persons)]
data[,freq := infections/persons]
data[freq==max(freq)]
# Load of data
earinfect <- read.table("Assignment 2/earinfect.txt", header = T)
head(earinfect)
earinfect$swimmer <- factor(earinfect$swimmer)
earinfect$location <- factor(earinfect$location)
earinfect$age <- factor(earinfect$age, ordered = T)
earinfect$sex <- factor(earinfect$sex)
earinfect$healthy <- earinfect$persons - earinfect$infections
# Plots -------------------------------------------------------------------
hist(earinfect$infections)
ggpairs(earinfect)
ggplot(earinfect) +
geom_histogram(aes(x = infections, y = after_stat(density)), bins = 15,  colour = "white", position = "identity", alpha = 0.4) +
theme_bw() +
labs(y = "", colour = "Distribution", title = "Earinfections")
ggsave(file.path(getwd(), "Assignment 2/figs/earinfect.png"), width = 20, height = 10, units = "cm")
# Is there any difference in mean and variance of infections variable?
mean(earinfect$infections)
var(earinfect$infections)
fit.pois <- glm(infections ~ offset(persons) * age * sex * location * swimmer,
data = earinfect, family = poisson(link = 'log'))
fit.pois_full <- fit.pois
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
coefficients(fit.pois)
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
# Drop the largest interaction
fit.pois <- update(fit.pois, .~.-age:sex:location:swimmer)
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
length(coefficients(fit.pois))
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
fit.pois <- update(fit.pois, .~.-age:sex:swimmer)
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
length(coefficients(fit.pois))
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
fit.pois <- update(fit.pois, .~.-sex:location:swimmer)
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
length(coefficients(fit.pois))
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
fit.pois <- update(fit.pois, .~.-age:location:swimmer)
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
length(coefficients(fit.pois))
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
fit.pois <- update(fit.pois, .~.-age:swimmer)
1 - pchisq(fit.pois$deviance, df = fit.pois$df.residual)
length(coefficients(fit.pois))
anova(fit.pois, test = "Chisq")
drop1(fit.pois, test = "Chisq")
# Looks good now
summary(fit.pois)
# Compare the reduced with full model
anova(fit.pois, fit.pois_full, test = "Chisq")
# Residual plot analysis -------------------------------------------------------
e.data <- earinfect
e.data$residuals <- fit.pois$residuals
e.data$leverage <- hatvalues(fit.pois)
# Residual analysis
e.data$pred <- predict(fit.pois)
e.data$pearson <- residuals(fit.pois, type = "pearson")
sigma_sq <- fit.pois$deviance / (dim(e.data)[1] - length(coefficients(fit.pois)))
e.data$stdpearson <- e.data$pearson/sqrt(sigma_sq*(1-e.data$leverage))
par(mfrow = c(2, 2))
plot(fit.pois)
## Forestplot
plot_summs(fit.pois)
fit.nb <- glm.nb(infections ~ persons + age * sex * location * swimmer,
data = earinfect)
fit.nb_full <- fit.nb
1 - pchisq(fit.nb$deviance, df = fit.nb$df.residual)
# Check whether residuals are i.i.d.
ggplot(e.data)+
geom_boxplot(aes(x = sex, y = pearson, fill = sex))+
theme_bw()+
ggtitle("Residual variation difference between genders")
fit.nb <- glm.nb(infections ~ persons + age * sex * location * swimmer,
data = earinfect)
fit.nb_full <- fit.nb
earinfect
str(earinfect)
